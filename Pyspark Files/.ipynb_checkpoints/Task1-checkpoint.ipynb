{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer \n",
    "from pyspark.ml.feature import HashingTF, IDF,Word2Vec\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import when, lit,udf,col\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD,LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "import pandas as pd\n",
    "import csv\n",
    "from csv import reader,writer\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/anand/Downloads/UB/Sem2/DIc/A3/dic487-587/train.csv')\n",
    "data_test = pd.read_csv('/Users/anand/Downloads/UB/Sem2/DIc/A3/dic487-587/test.csv')\n",
    "df2 = spark.createDataFrame(data)\n",
    "df3 = spark.createDataFrame(data_test)\n",
    "#print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Model1\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "#add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\",\"a\"] \n",
    "stopwordsRemoverList = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").loadDefaultStopWords(\"english\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stopwordsRemoverList)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "##wordsData=regexTokenizer.transform(df2)\n",
    "##featurizedData = hashingTF.transform(wordsData)\n",
    "##\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"featuresidf\")\n",
    "##idfModel = idf.fit(featurizedData)\n",
    "##df2 = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec = Word2Vec(vectorSize=300, minCount=0,inputCol='filtered', outputCol='word2vec_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model23 = word2vec.fit(stopwordsRemover.transform(regexTokenizer.transform(df2)))\n",
    "\n",
    "# result = model23.transform(stopwordsRemover.transform(regexTokenizer.transform(df2)))\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('/Users/anand/Downloads/UB/Sem2/DIc/A3/dic487-587/mapping.csv')\n",
    "mapping = spark.createDataFrame(mapping)\n",
    "mg = mapping.select(\"0\",\"Unnamed: 0\").rdd.collectAsMap()\n",
    "count = mapping.count()\n",
    "# Converting Genres to respective mapped ids\n",
    "from pyspark.sql.types import *\n",
    "def label_to_id(g):\n",
    "    x=[]\n",
    "    print(g)\n",
    "    for i in g[1:-1].split(\",\"):\n",
    "        x.append(mg.get(i.strip()[1:-1]))\n",
    "    x.sort()\n",
    "    return x\n",
    "udff=UserDefinedFunction(label_to_id,ArrayType(IntegerType()))\n",
    "\n",
    "df2=df2.withColumn(\"lab\",udff(\"genre\"))\n",
    "#df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting mapped ids to one hot encoding \n",
    "def one_hot_list(g):\n",
    "    y=1\n",
    "    x=[]\n",
    "    for i in range(0,count):\n",
    "        x.append(0)\n",
    "    for i in g:\n",
    "        x[i]=1 \n",
    "    return x\n",
    "udff=UserDefinedFunction(one_hot_list,ArrayType(IntegerType()))\n",
    "traindata = df2.withColumn(\"label\",udff(\"lab\"))\n",
    "#traindata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model1\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model2\n",
    "#pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF,idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model3\n",
    "#pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover,word2vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train!\n"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(df2)\n",
    "print('train!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit2 = pipeline.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pipelineFit.transform(traindata)\n",
    "testdataset = pipelineFit2.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_raw=[]\n",
    "for i in range(0,count):\n",
    "    def prep(line):\n",
    "        return LabeledPoint(line.label[i], MLLibVectors.fromML(line.features))\n",
    "    prepd_data = dataset.rdd.map(prep)\n",
    "    model = LogisticRegressionWithLBFGS.train(prepd_data)\n",
    "    with open('task1model'+str(i)+'.pkl','wb') as fid:\n",
    "        pickle.dump(model, fid)\n",
    "    labelsAndPreds = testdataset.rdd.map(lambda p: (p.movie_id, model.predict(MLLibVectors.fromML(p.features))))\n",
    "    pred_raw.append(labelsAndPreds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#from csv import reader,writer\n",
    "#import pandas as pd\n",
    "column_name = ['movie_id', 'predictions']\n",
    "data = pd.DataFrame(columns = column_name)\n",
    "\n",
    "#with open('out1.csv','a') as writeobj:\n",
    "    #csv_writer= writer(write_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pred_raw\n",
    "preds=[]\n",
    "ids = []\n",
    "for j in range(0,len(x[0])):\n",
    "    preds=[]\n",
    "    ids.append(str(x[0][j][0]))\n",
    "    for i in range(0,count):\n",
    "        preds.append(str(x[i][j][1]))\n",
    "    temp = list(map(int, preds))\n",
    "    #print(temp)\n",
    "    temp2 =  ' '.join(map(str, temp))\n",
    "    #print(int(ids[j]))\n",
    "    temp = pd.DataFrame([[ids[j], temp2]], columns = column_name)\n",
    "    data = data.append(temp)\n",
    "\n",
    "#data.to_csv('/Users/anand/Downloads/UB/Sem2/DIc/A3/result142.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/anand/Downloads/UB/Sem2/DIc/A3/result614.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
