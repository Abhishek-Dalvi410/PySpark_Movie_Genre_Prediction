{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.mllib import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD,LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"My Demo\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET THERE BE TRAINING DATA\n",
    "train_csv = pd.read_csv(\"/Users/anand/Downloads/UB/Sem2/DIc/A3/dic487-587/train.csv\")\n",
    "# LET THERE BE Test DATA\n",
    "test_csv = pd.read_csv(\"/Users/anand/Downloads/UB/Sem2/DIc/A3/dic487-587/test.csv\")\n",
    "# LET THERE BE Mapping\n",
    "map_csv = pd.read_csv(\"/Users/anand/Downloads/UB/Sem2/DIc/A3/dic487-587/mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET THERE BE DATAFRAME\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enable\", \"true\")\n",
    "train_set = spark.createDataFrame(train_csv)\n",
    "test_set = spark.createDataFrame(test_csv)\n",
    "mapping = spark.createDataFrame(map_csv)\n",
    "count = mapping.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|movie_id|          movie_name|                plot|               genre|             lab|               label|\n",
      "+--------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|23890098|          Taxi Blues|Shlykov, a hard-w...|['World cinema', ...|          [0, 5]|[1, 0, 0, 0, 0, 1...|\n",
      "|31186339|    The Hunger Games|The nation of Pan...|['Action/Adventur...|  [0, 4, 10, 17]|[1, 0, 0, 0, 1, 0...|\n",
      "|20663735|          Narasimham|Poovalli Induchoo...|['Musical', 'Acti...|      [0, 4, 16]|[1, 0, 0, 0, 1, 0...|\n",
      "| 2231378|  The Lemon Drop Kid|The Lemon Drop Ki...|          ['Comedy']|             [1]|[0, 1, 0, 0, 0, 0...|\n",
      "|  595909|   A Cry in the Dark|Seventh-day Adven...|['Crime Fiction',...|       [0, 5, 6]|[1, 0, 0, 0, 0, 1...|\n",
      "| 5272176|            End Game|The president is ...|['Action/Adventur...|   [0, 3, 4, 10]|[1, 0, 0, 1, 1, 0...|\n",
      "| 1952976|          Dark Water|{{plot}} The film...|['Thriller', 'Dra...|       [0, 3, 7]|[1, 0, 0, 1, 0, 0...|\n",
      "|24225279|                Sing|The story begins ...|           ['Drama']|             [0]|[1, 0, 0, 0, 0, 0...|\n",
      "| 2462689|       Meet John Doe|Infuriated at bei...|['Black-and-white...|[0, 1, 2, 8, 19]|[1, 1, 1, 0, 0, 0...|\n",
      "|20532852|Destination Meatball|A line of people ...|['Animation', 'Sh...|    [12, 13, 15]|[0, 0, 0, 0, 0, 0...|\n",
      "|15401493|    Husband for Hire|Lola  attempts to...|          ['Comedy']|             [1]|[0, 1, 0, 0, 0, 0...|\n",
      "|18188932|         Up and Down|Milan and Goran a...|['Crime Fiction',...|    [0, 1, 5, 6]|[1, 1, 0, 0, 0, 1...|\n",
      "| 2940516|Ghost In The Noon...|Bumbling pirate c...|          ['Comedy']|             [1]|[0, 1, 0, 0, 0, 0...|\n",
      "| 1480747|       House Party 2|{{plot}} Followin...|          ['Comedy']|             [1]|[0, 1, 0, 0, 0, 0...|\n",
      "|24448645|Forest of the Dam...|Despite Lucy's re...|          ['Horror']|             [7]|[0, 0, 0, 0, 0, 0...|\n",
      "|15072401|Charlie Chan's Se...|Alan Colby, heir ...|['Crime Fiction',...|   [3, 6, 7, 18]|[0, 0, 0, 1, 0, 0...|\n",
      "| 4018288|     The Biggest Fan|Debbie's favorite...|           ['Drama']|             [0]|[1, 0, 0, 0, 0, 0...|\n",
      "| 4596602|      Ashes to Ashes|Ashes to Ashes is...|['Crime Fiction',...| [2, 3, 4, 6, 9]|[0, 0, 1, 1, 1, 0...|\n",
      "|15224586|        Green Dragon|The film follows ...|  ['Indie', 'Drama']|          [0, 9]|[1, 0, 0, 0, 0, 0...|\n",
      "|15585766|  The Rats of Tobruk|Three friends are...|           ['Drama']|             [0]|[1, 0, 0, 0, 0, 0...|\n",
      "+--------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding\n",
    "mg = mapping.select(\"0\",\"Unnamed: 0\").rdd.collectAsMap()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def label_to_id(g):\n",
    "    x=[]\n",
    "    print(g)\n",
    "    for i in g[1:-1].split(\",\"):\n",
    "        x.append(mg.get(i.strip()[1:-1]))\n",
    "    x.sort()\n",
    "    return x\n",
    "udff=f.udf(label_to_id,ArrayType(IntegerType()))\n",
    "\n",
    "train_set = train_set.withColumn(\"lab\",udff(\"genre\"))\n",
    "\n",
    "def one_hot_list(g):\n",
    "    y=1\n",
    "    x=[]\n",
    "    for i in range(0,count):\n",
    "        x.append(0)\n",
    "    for i in g:\n",
    "        x[i]=1 \n",
    "    return x\n",
    "\n",
    "udff=f.udf(one_hot_list,ArrayType(IntegerType()))\n",
    "\n",
    "traindata=train_set.withColumn(\"label\",udff(\"lab\"))\n",
    "\n",
    "#traindata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordsRemoverList = ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any',\n",
    "#                  'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "#                  'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\",\n",
    "#                  'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few',\n",
    "#                  'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven',\n",
    "#                  \"haven't\", 'having', 'he', 'her', 'here','hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "#                  'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll',\n",
    "#                  'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', \n",
    "#                  'needn',\"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "#                  'other','our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\",\n",
    "#                  'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than',\n",
    "#                  'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these',\n",
    "#                  'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was',\n",
    "#                  'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while',\n",
    "#                  'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\",\n",
    "#                  \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsRemoverList = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").loadDefaultStopWords(\"english\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stopwordsRemoverList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vectorSize=300, minCount=0,inputCol='filtered', outputCol='word2vec_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data done!\n",
      "Test data done\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, word2vec])\n",
    "fitted_pipeline_train = pipeline.fit(train_set)\n",
    "print(\"Training data done!\")\n",
    "fitted_pipeline_test = pipeline.fit(test_set)\n",
    "print(\"Test data done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_for_train = fitted_pipeline_train.transform(traindata)\n",
    "ready_for_test = fitted_pipeline_test.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=[]\n",
    "for i in range(0,count):\n",
    "    def parsePoint(line):\n",
    "        return LabeledPoint(line.label[i], MLLibVectors.fromML(line.word2vec_features))\n",
    "    parsedData = ready_for_train.rdd.map(parsePoint)\n",
    "    model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "    with open('task3model'+str(i)+'.pkl','wb') as fid:\n",
    "        pickle.dump(model, fid)\n",
    "    labelsAndPreds = ready_for_test.rdd.map(lambda p: \n",
    "                                            (p.movie_id, model.predict(MLLibVectors.fromML(p.word2vec_features))))\n",
    "    x.append(labelsAndPreds.collect())\n",
    "    #print(f\"Done for count: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the dataset\n",
    "import csv\n",
    "from csv import reader,writer\n",
    "import pandas as pd\n",
    "column_name = ['movie_id', 'predictions']\n",
    "data = pd.DataFrame(columns = column_name)\n",
    "\n",
    "#with open('out1.csv','a') as writeobj:\n",
    "    #csv_writer= writer(write_obj)\n",
    "preds=[]\n",
    "ids = []\n",
    "for j in range(0,len(x[0])):\n",
    "    preds=[]\n",
    "    ids.append(str(x[0][j][0]))\n",
    "    for i in range(0,count):\n",
    "        preds.append(str(x[i][j][1]))\n",
    "    temp = list(map(int, preds))\n",
    "    #print(temp)\n",
    "    temp2 =  ' '.join(map(str, temp))\n",
    "    #print(int(ids[j]))\n",
    "    temp = pd.DataFrame([[ids[j], temp2]], columns = column_name)\n",
    "    data = data.append(temp)\n",
    "\n",
    "data.to_csv('/Users/anand/Downloads/UB/Sem2/DIc/A3/result713.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
